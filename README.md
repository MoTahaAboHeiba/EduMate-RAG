# ğŸ“ EduMate RAG System

[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status: Production Ready](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)](#)

A **production-ready Retrieval-Augmented Generation (RAG)** backend for the EduMate Flutter application. Powered by **LangChain**, **ChromaDB**, **Groq's free LLM API**, and built with **FastAPI** for seamless academic Q&A.

---

## ğŸ“– Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Conversation Examples](#conversation-examples)
- [Project Structure](#project-structure)
- [How RAG Works](#how-rag-works)
- [Troubleshooting](#troubleshooting)
- [Performance Metrics](#performance-metrics)
- [Contributing](#contributing)
- [License](#license)

---

## Overview

**EduMate RAG** is an intelligent, conversational question-answering system designed for Egyptian universities. It empowers students to ask questions about their course materials in both Arabic and English, retrieving accurate information from indexed PDFs and generating contextual answers using advanced AI.

### Key Innovation
Answers are **grounded exclusively in your course materials**â€”no hallucinations, only verified facts from your PDFs. The system maintains conversation context across multiple turns, enabling natural dialogue about course content.

---

##  Features

- **ğŸ“š PDF-Based Q&A** - Answer questions only from indexed course materials (no external data)
- **ğŸ’¬ Multi-Turn Conversations** - Remember context across questions for natural dialogue
- **ğŸš€ Instant Retrieval** - ChromaDB enables sub-second semantic search across thousands of documents
- **ğŸ¤– AI-Powered Generation** - Groq's Llama 3.3 70B for high-quality, contextual answers
- **ğŸŒ Multilingual Support** - Seamlessly handles Arabic and English questions and documents
- **ğŸ“Š Source Attribution** - Every answer includes source document references for verification
- **ğŸ” Security-First** - Secrets stored in `.env`, never committed to Git
- **âš¡ Zero-Cost Inference** - Uses Groq's free tier (no LLM hosting costs)
- **ğŸ“± Flutter-Ready** - RESTful API endpoints optimized for mobile integration
- **ğŸ“ˆ Production-Ready** - Professional code structure, comprehensive error handling, detailed logging
- **âš™ï¸ Intelligent Caching** - Efficient indexing with vector embeddings for fast retrieval
- **ğŸ§  Context-Aware** - Understands references to previous questions

---

##  Tech Stack

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Language** | Python | 3.9+ | Core application |
| **Web Framework** | FastAPI | 0.109+ | REST API server |
| **ASGI Server** | Uvicorn | 0.27+ | HTTP server |
| **LLM Framework** | LangChain | 0.1.20+ | RAG orchestration |
| **LLM Provider** | Groq | -- | Free cloud LLM API |
| **LLM Model** | Llama 3.3 70B | Latest | Answer generation |
| **Vector DB** | ChromaDB | 0.4.22+ | Semantic search |
| **PDF Processing** | PyPDF | 4.0.1+ | Text extraction |
| **Config Management** | python-dotenv | 1.0+ | Environment variables |
| **Version Control** | Git | -- | Code versioning |

---

##  Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Flutter Mobile App                    â”‚
â”‚              (Student Interface Layer)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/REST
                     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â–¼                                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
        â”‚   FastAPI Server (Port 8000)â”‚                    â”‚
        â”‚  â”œâ”€ POST /api/query         â”‚                    â”‚
        â”‚  â”œâ”€ GET /api/conversation/* â”‚                    â”‚
        â”‚  â””â”€ POST /api/index         â”‚                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                 â”‚                                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
        â”‚   RAG Pipeline (LangChain)                â”‚      â”‚
        â”‚  â”Œâ”€ Conversation Memory  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
        â”‚  â”œâ”€ PDF Retrieval (ChromaDB)              â”‚      â”‚
        â”‚  â””â”€ LLM Generation (Groq)                 â”‚      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                 â”‚                                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
        â”‚   Data Layer                              â”‚      â”‚
        â”‚  â”œâ”€ ChromaDB (Vector DB)                  â”‚      â”‚
        â”‚  â”œâ”€ PDF Embeddings                        â”‚      â”‚
        â”‚  â””â”€ Conversation History                  â”‚      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚   External Services                          â”‚   â”‚
        â”‚  â”œâ”€ Groq API (LLM Inference)                 â”‚   â”‚
        â”‚  â””â”€ (No storage of personal data)            â”‚   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
```

---

##  Prerequisites

### System Requirements
- **Python:** 3.9 or higher ([Download](https://www.python.org/downloads/))
- **Git:** Version control ([Download](https://git-scm.com/))
- **RAM:** Minimum 4GB (8GB recommended for optimal performance)
- **Storage:** 5GB+ free space for dependencies and vector database
- **Internet:** Required for Groq API calls

### Accounts & Keys
- **Groq API Key:** Free from [Groq Console](https://console.groq.com) (required)
- **Course PDFs:** Your academic materials in PDF format

### Optional
- **Tesseract OCR:** For image-based PDFs ([Installation Guide](https://github.com/UB-Mannheim/tesseract/wiki))

---

##  Installation

### Step 1: Clone Repository

```bash
git clone <your-repository-url>
cd EduMate-RAG
```

### Step 2: Create Virtual Environment

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**macOS/Linux:**
```bash
python -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

Expected output:
```
Successfully installed langchain-0.1.20 chromadb-0.4.22 fastapi-0.109.0 ...
```

### Step 4: Configure Environment

1. Copy template:
```bash
cp .env.example .env
```

2. Edit `.env` with your Groq API key:
```bash
# Open .env in your text editor
GROQ_API_KEY=gsk_your_actual_key_here
```

### Step 5: Add Course PDFs

Place PDF files in:
```
assets/course_pdfs/
â”œâ”€â”€ Course_1.pdf
â”œâ”€â”€ Course_2.pdf
â”œâ”€â”€ Math_Fundamentals.pdf
â””â”€â”€ ...
```

### Step 6: Verify Installation

```bash
python test_groq_direct.py
```

**Expected output:**
```
ğŸ”„ Testing Groq connection...
âœ… Groq is working!
Response: content='...'
```

### Step 7: Index PDFs

```bash
python main.py
```

In another terminal:
```bash
curl -X POST http://localhost:8000/api/index
```

---

## Configuration

### .env File

```bash
# Groq API Configuration
GROQ_API_KEY=gsk_your_key_here              # Get from console.groq.com
GROQ_MODEL=llama-3.3-70b-versatile          # Latest recommended model

# ChromaDB Configuration
CHROMA_DB_PATH=./assets/chroma_db           # Vector database location
PDF_FOLDER_PATH=./assets/course_pdfs        # PDF source folder

# API Configuration
API_HOST=localhost                           # Server host
API_PORT=8000                               # Server port
DEBUG=True                                  # Enable debug logging
```

### Get Groq API Key

1. Go to [https://console.groq.com](https://console.groq.com)
2. Sign up (free, takes 1 minute)
3. Navigate to "API Keys" section
4. Click "Create API Key"
5. Copy the key starting with `gsk_`
6. Paste into `.env` file

### Available LLM Models

| Model | Speed | Quality | Cost | Best For |
|-------|-------|---------|------|----------|
| `llama-3.1-8b-instant` | â­â­â­â­â­ Very Fast | â­â­â­ Good | Free | Budget/Speed |
| `llama-3.3-70b-versatile` | â­â­â­ Medium | â­â­â­â­â­ Excellent | Free | **Recommended** |
| `llama-2-70b-4096` | â­â­â­â­ Fast | â­â­â­â­ Great | Free | Alternative |

---

## Usage

### Start Server

```bash
python main.py
```

**Expected output:**
```
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     Application startup complete
```

### Index PDFs

```bash
curl -X POST http://localhost:8000/api/index
```

**Response:**
```json
{
  "status": "success",
  "message": "PDFs indexed successfully",
  "documents_indexed": 13096
}
```

### Query with Conversation

**Question 1:**
```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is a prerequisite?"}'
```

**Question 2 (System remembers!):**
```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Tell me more about that"}'
```

### Interactive Testing

Open FastAPI Swagger UI:
```
http://localhost:8000/docs
```

- Click on any endpoint
- Click "Try it out"
- Enter your data
- Click "Execute"

---

## API Documentation

### Base URL
```
http://localhost:8000
```

### Endpoints

#### 1. GET `/` - Root Endpoint
**Purpose:** Verify API is running

**Response:**
```json
{
  "message": "EduMate RAG API is running!",
  "version": "2.0.0",
  "features": ["PDF Q&A", "Conversation Memory", "Source Attribution"]
}
```

---

#### 2. GET `/health` - Health Check
**Purpose:** Check system health and vector store status

**Response:**
```json
{
  "status": "healthy",
  "model": "llama-3.3-70b-versatile",
  "vector_store": {
    "collection": "course_materials",
    "documents_indexed": 13096
  },
  "features": {
    "conversation_memory": true,
    "multi_turn_support": true,
    "context_awareness": true
  }
}
```

---

#### 3. POST `/api/query` - Query with Conversation
**Purpose:** Ask questions about course materials (with conversation memory)

**Request:**
```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the prerequisites for CS101?"
  }'
```

**Response:**
```json
{
  "question": "What are the prerequisites for CS101?",
  "answer": "Based on the course materials, the prerequisites for CS101 are: Data Structures (CS100) and Discrete Mathematics (MATH101)...",
  "sources": ["Computer Science - First Year 2023"],
  "num_context_docs": 3,
  "conversation_turn": 1
}
```

**Parameters:**
- `question` (string): Student's question

**Returns:**
- `question`: Echo of the question
- `answer`: AI-generated answer from PDFs
- `sources`: Source documents used
- `num_context_docs`: Number of documents retrieved
- `conversation_turn`: Which turn in conversation (1, 2, 3...)

---

#### 4. POST `/api/index` - Index PDFs
**Purpose:** Load and index all PDFs into vector database

**Request:**
```bash
curl -X POST http://localhost:8000/api/index
```

**Response:**
```json
{
  "status": "success",
  "message": "PDFs indexed successfully",
  "documents_indexed": 13096
}
```

---

#### 5. GET `/api/conversation/history` - Get Conversation History
**Purpose:** Retrieve full conversation history

**Request:**
```bash
curl http://localhost:8000/api/conversation/history
```

**Response:**
```json
{
  "total_turns": 3,
  "messages": [
    {
      "role": "student",
      "content": "What is a prerequisite?"
    },
    {
      "role": "assistant",
      "content": "A prerequisite is a course or requirement..."
    },
    {
      "role": "student",
      "content": "Tell me more"
    },
    {
      "role": "assistant",
      "content": "Based on our previous discussion, prerequisites..."
    }
  ]
}
```

---

#### 6. POST `/api/conversation/clear` - Clear Conversation
**Purpose:** Start fresh conversation

**Request:**
```bash
curl -X POST http://localhost:8000/api/conversation/clear
```

**Response:**
```json
{
  "status": "success",
  "message": "Conversation memory cleared",
  "note": "Next question will start a new conversation"
}
```

---

#### 7. GET `/api/conversation/info` - Conversation Statistics
**Purpose:** Get current conversation stats

**Request:**
```bash
curl http://localhost:8000/api/conversation/info
```

**Response:**
```json
{
  "total_turns": 3,
  "total_messages": 6,
  "status": "active"
}
```

---

## Conversation Examples

### Example 1: Multi-Turn Academic Discussion

```
Q1: "What is artificial intelligence?"
A1: "Artificial intelligence refers to the simulation of human intelligence processes by machines, particularly computer systems. These processes include learning, reasoning, and self-correction. [Source: AI Fundamentals Course]"

Q2: "Tell me more about machine learning"
A2: "Based on our discussion about AI, machine learning is a subset of artificial intelligence where systems learn from data and improve without explicit programming. It's one of the key applications of AI discussed in the course materials. [Source: AI Fundamentals Course]"

Q3: "How is it different from deep learning?"
A3: "Machine learning and deep learning are related but different. Machine learning is a broader field, while deep learning is a specific subset that uses neural networks with multiple layers. [Source: AI Fundamentals Course]"
```

### Example 2: Arabic Questions

```
Q: "Ù…Ø§ Ù‡ÙŠ Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ø§Ø¯Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©ØŸ"
A: "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ù‚Ø±Ø±ØŒ Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ø§Ø¯Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ù‡ÙŠ: Ù…Ù‚Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŒ ÙˆØ§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„Ù…Ù†ÙØµÙ„Ø©ØŒ ÙˆÙ…Ù‡Ø§Ø±Ø§Øª Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ..."
```

---

##  Project Structure

```
EduMate-RAG/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                    # Entry point - starts the server
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md                  # This file
â”œâ”€â”€ ğŸ“„ .env.example               # Environment template (safe)
â”œâ”€â”€ ğŸ“„ .env                       # Your secrets (NOT in Git)
â”œâ”€â”€ ğŸ“„ .gitignore                 # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“ src/                       # Source code (core logic)
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ config.py              # Configuration loader
â”‚   â”œâ”€â”€ ğŸ“„ pdf_loader.py          # PDF extraction & chunking
â”‚   â”œâ”€â”€ ğŸ“„ vector_store.py        # ChromaDB integration
â”‚   â”œâ”€â”€ ğŸ“„ rag_chain.py           # RAG pipeline with memory
â”‚   â””â”€â”€ ğŸ“ api/
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â””â”€â”€ ğŸ“„ main.py            # FastAPI endpoints
â”‚
â”œâ”€â”€ ğŸ“ assets/                    # Data & storage
â”‚   â”œâ”€â”€ ğŸ“ course_pdfs/           # Your course PDF files
â”‚   â””â”€â”€ ğŸ“ chroma_db/             # Vector database (auto-created)
â”‚
â”œâ”€â”€ ğŸ“ tests/                     # Test & verification scripts
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ test_groq_direct.py    # Test Groq connection
â”‚   â”œâ”€â”€ ğŸ“„ test_embeddings.py     # Test embeddings
â”‚   â””â”€â”€ ğŸ“„ verify_chromadb.py     # Verify vector database
â”‚
â””â”€â”€ ğŸ“ venv/                      # Python virtual environment
    â”œâ”€â”€ ğŸ“ Scripts/ (Windows)
    â”œâ”€â”€ ğŸ“ bin/ (macOS/Linux)
    â””â”€â”€ ...
```

---

##  How RAG Works

### RAG = Retrieval-Augmented Generation

The system operates in **3 key stages**:

#### **Stage 1: Retrieval**
```
Student Query: "What is a prerequisite?"
                    â†“
        Search embeddings in ChromaDB
                    â†“
        Find top 3 similar PDF chunks
                    â†“
    Retrieve: ["A prerequisite is...", "Prerequisites include...", "Before taking..."]
```

#### **Stage 2: Context Creation**
```
Retrieved chunks are combined:

"[Computer Science PDF] A prerequisite is a course or skill required before enrollment.
[Admin PDF] Prerequisites ensure students have necessary background knowledge.
[Curriculum PDF] Each course lists its prerequisites in the course description."
```

#### **Stage 3: Generation**
```
Context + Question sent to Groq LLM:

Input: {context} + "What is a prerequisite?"
           â†“
    Llama 3.3 70B processes
           â†“
Output: "Based on the course materials, a prerequisite is a course or requirement that must be completed before taking another course..."
```

### Conversation Memory Integration

```
Turn 1: Q1 â†’ Retrieve(Q1) + Generate(Q1) â†’ Save Q1+A1 to Memory
Turn 2: Q2 â†’ Retrieve(Q2) + Memory(Q1+A1) + Generate(Q2) â†’ Save Q2+A2 to Memory
Turn 3: Q3 â†’ Retrieve(Q3) + Memory(Q1+A1+Q2+A2) + Generate(Q3) â†’ Save Q3+A3 to Memory
```

This enables the system to understand references like "Tell me more," "Explain that further," etc.

---

## Testing

### Test Groq Connection
```bash
python test_groq_direct.py
```

### Test Embeddings
```bash
python test_embeddings.py
```

### Verify ChromaDB
```bash
python verify_chromadb.py
```

### Test All Endpoints
```bash
# Health check
curl http://localhost:8000/health

# Query
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is data structure?"}'

# Conversation history
curl http://localhost:8000/api/conversation/history

# Clear conversation
curl -X POST http://localhost:8000/api/conversation/clear
```

---

##  Troubleshooting

### Issue: "GROQ_API_KEY not set"
**Solution:**
1. Check `.env` file exists in project root
2. Verify key is set (not `your_key_here` placeholder)
3. No extra spaces around key
4. Restart server

---

### Issue: "No PDFs found"
**Solution:**
1. Verify PDFs in `assets/course_pdfs/`
2. Check file extension is `.pdf` (lowercase)
3. Ensure PDFs aren't corrupted
4. Try with a simple PDF first

---

### Issue: "ChromaDB error"
**Solution:**
```bash
# Delete old database
rm -rf assets/chroma_db

# Restart server
python main.py

# Re-index
curl -X POST http://localhost:8000/api/index
```

---

### Issue: "Connection refused" to localhost:8000
**Solution:**
1. Ensure server is running: `python main.py`
2. Check port isn't in use
3. Try different port in `.env`: `API_PORT=8001`

---

### Issue: Slow query responses
**Causes & Solutions:**
- **Large PDFs:** Response time is normal (2-5 seconds)
- **First query:** Model loads on first use (normal)
- **Network latency:** Groq servers responding normally

---

##  Performance Metrics

| Metric | Value | Notes |
|--------|-------|-------|
| **Indexing Speed** | 10+ chunks/sec | With telemetry disabled |
| **Query Response Time** | 1-3 seconds | Includes retrieval + LLM generation |
| **Vector Search Speed** | <100ms | Sub-second for 13,000+ documents |
| **Memory Usage** | ~1-2GB | Running with full vector DB |
| **Concurrent Users** | Limited by Groq API rate limits | Free tier handles typical usage |
| **Model Size** | 70 billion parameters | Llama 3.3 70B |
| **Embedding Dimension** | 384 | sentence-transformers model |

---

## Security Considerations

- âœ… API Keys in `.env` (not in Git)
- âœ… CORS enabled for Flutter (can be restricted)
- âœ… Input validation on all endpoints
- âœ… Error messages don't expose sensitive data
- âš ï¸ No authentication implemented (add before production)
- âš ï¸ No rate limiting (add before public deployment)

---

##  Deployment

### Local Development
```bash
python main.py
```

### Production Deployment

1. **Use production ASGI server:**
```bash
pip install gunicorn
gunicorn -w 4 -k uvicorn.workers.UvicornWorker src.api.main:app
```

2. **Set environment variables:**
```bash
export GROQ_API_KEY=gsk_...
export API_HOST=0.0.0.0
export DEBUG=False
```

3. **Add authentication:**
- Implement JWT tokens
- Add API key validation
- Restrict CORS origins

4. **Add monitoring:**
- Log all queries
- Monitor API response times
- Track vector DB size

---

##  Additional Resources

- **FastAPI:** https://fastapi.tiangolo.com/
- **LangChain:** https://python.langchain.com/
- **ChromaDB:** https://docs.trychroma.com/
- **Groq API:** https://console.groq.com/docs/
- **RAG Concepts:** https://aws.amazon.com/blogs/machine-learning/
- **Embeddings:** https://huggingface.co/spaces/mteb/leaderboard

---

##  Contributing

Contributions welcome! To contribute:

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m "Add amazing feature"`
4. Push: `git push origin feature/amazing-feature`
5. Open Pull Request

---

##  License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file for details.

---

##  Author

**EduMate Development Team**


---

##  Support & Contact

For issues, questions, or suggestions:

1. Check [Troubleshooting](#troubleshooting) section
2. Review [API Documentation](#api-documentation)
3. Check server logs for errors
4. Open an issue on GitHub

---

##  Educational Value

This project demonstrates:

- âœ… **RAG Architecture** - Retrieval-Augmented Generation implementation
- âœ… **Vector Databases** - Semantic search with embeddings
- âœ… **LLM Integration** - Using Groq API for inference
- âœ… **Conversational AI** - Multi-turn memory management
- âœ… **API Design** - RESTful endpoint design with FastAPI
- âœ… **Production Practices** - Error handling, logging, security
- âœ… **PDF Processing** - Text extraction and chunking
- âœ… **Version Control** - Git workflow

---

##  Roadmap

### V2.1 (Next)
- [ ] User authentication & JWT tokens
- [ ] Rate limiting per user
- [ ] Query analytics & logging
- [ ] Answer rating system

### V2.2
- [ ] Web admin dashboard
- [ ] Multiple conversation sessions per user
- [ ] PDF upload via API
- [ ] Full-text search fallback

### V3.0
- [ ] Mobile app integration (Flutter)
- [ ] Multilingual UI support
- [ ] Advanced analytics
- [ ] Cloud deployment templates

---

## âœ¨ Acknowledgments

- **Groq** for free LLM API access
- **LangChain** for RAG orchestration
- **ChromaDB** for vector storage
- **FastAPI** for web framework


---

##  Changelog

### v2.0.0 (Current)
- âœ… Conversational RAG with memory
- âœ… Multi-turn context awareness
- âœ… Improved error handling
- âœ… Professional API documentation

### v1.0.0 (Initial)
- âœ… Basic RAG pipeline
- âœ… Single-question support
- âœ… PDF indexing

---

**Made with â¤ï¸ for education | Last updated: December 3, 2025**

---

## Quick Start Command

```bash
# Clone
git clone <url>
cd EduMate-RAG

# Setup
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
cp .env.example .env

# Configure
# Edit .env with your Groq API key

# Run
python main.py

# In another terminal
curl -X POST http://localhost:8000/api/index  # Index PDFs
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Your question here"}'
```
